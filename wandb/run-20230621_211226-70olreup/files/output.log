/home/andrea/miniconda3/envs/cla-transformer/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
  rank_zero_warn(
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
  | Name            | Type             | Params
-----------------------------------------------------
0 | gloss_encoder   | GlossEncoder     | 108 M
1 | context_encoder | ContextEncoder   | 108 M
2 | loss            | CrossEntropyLoss | 0
3 | cc              | CosineSimilarity | 0
-----------------------------------------------------
216 M     Trainable params
0         Non-trainable params
216 M     Total params
866.482   Total estimated model params size (MB)
/home/andrea/miniconda3/envs/cla-transformer/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/andrea/miniconda3/envs/cla-transformer/lib/python3.10/site-packages/pytorch_lightning/utilities/data.py:120: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-process data loading (when num_workers > 1), `__len__` could be inaccurate if each worker is not configured independently to avoid having duplicate data.
  rank_zero_warn(
Sanity Checking DataLoader 0:  50%|█████████████████████████████████████████████████████████████                                                             | 1/2 [00:00<00:00,  1.05it/s]
/home/andrea/miniconda3/envs/cla-transformer/lib/python3.10/site-packages/pytorch_lightning/utilities/data.py:76: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 8. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/home/andrea/miniconda3/envs/cla-transformer/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
Sanity Checking DataLoader 0: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.08it/s]-2851.7807006835938




































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































Epoch 0: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 2494/2494 [40:49<00:00,  1.02it/s, v_num=reup, train_loss=-891.]





















































Epoch 0: 100%|██████████████████████████████████████████| 2494/2494 [42:37<00:00,  1.03s/it, v_num=reup, train_loss=-891., train_loss_step=-, train_loss_epoch=-2.29e+3, val_loss=-3.17e+5]-6042924.629528046
Epoch 1:   0%|                                                     | 0/2494 [00:00<?, ?it/s, v_num=reup, train_loss=-891., train_loss_step=-, train_loss_epoch=-2.29e+3, val_loss=-3.17e+5]
/home/andrea/miniconda3/envs/cla-transformer/lib/python3.10/site-packages/pytorch_lightning/utilities/data.py:76: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.



























































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































Epoch 1: 100%|███████████████████████████████████████| 2494/2494 [37:13<00:00,  1.12it/s, v_num=reup, train_loss=-1.01e+3, train_loss_step=-, train_loss_epoch=-2.29e+3, val_loss=-6.04e+6]






















































Epoch 1: 100%|███████████████████████████████████████| 2494/2494 [39:02<00:00,  1.06it/s, v_num=reup, train_loss=-1.01e+3, train_loss_step=-, train_loss_epoch=-2.52e+3, val_loss=-3.49e+5]-6584210.388122559




































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































Epoch 2: 100%|███████████████████████████████████████| 2494/2494 [40:46<00:00,  1.02it/s, v_num=reup, train_loss=-1.01e+3, train_loss_step=-, train_loss_epoch=-2.52e+3, val_loss=-6.58e+6]






















































Epoch 2: 100%|███████████████████████████████████████| 2494/2494 [42:34<00:00,  1.02s/it, v_num=reup, train_loss=-1.01e+3, train_loss_step=-, train_loss_epoch=-2.53e+3, val_loss=-3.49e+5]-6673167.155227661



























































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































Epoch 3: 100%|███████████████████████████████████████| 2494/2494 [37:14<00:00,  1.12it/s, v_num=reup, train_loss=-1.01e+3, train_loss_step=-, train_loss_epoch=-2.53e+3, val_loss=-6.67e+6]






















































Epoch 3: 100%|████████████████████████████████████████| 2494/2494 [39:03<00:00,  1.06it/s, v_num=reup, train_loss=-1.01e+3, train_loss_step=-, train_loss_epoch=-2.53e+3, val_loss=-3.5e+5]-6683846.632369995





































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































Epoch 4: 100%|███████████████████████████████████████| 2494/2494 [40:56<00:00,  1.02it/s, v_num=reup, train_loss=-1.02e+3, train_loss_step=-, train_loss_epoch=-2.53e+3, val_loss=-6.68e+6]






















































Epoch 4: 100%|████████████████████████████████████████| 2494/2494 [42:46<00:00,  1.03s/it, v_num=reup, train_loss=-1.02e+3, train_loss_step=-, train_loss_epoch=-2.53e+3, val_loss=-3.5e+5]-6690953.43775177


























































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































Epoch 5: 100%|███████████████████████████████████████| 2494/2494 [37:24<00:00,  1.11it/s, v_num=reup, train_loss=-1.02e+3, train_loss_step=-, train_loss_epoch=-2.53e+3, val_loss=-6.69e+6]






















































Epoch 5: 100%|████████████████████████████████████████| 2494/2494 [39:13<00:00,  1.06it/s, v_num=reup, train_loss=-1.02e+3, train_loss_step=-, train_loss_epoch=-2.53e+3, val_loss=-3.5e+5]-6696035.403038025







































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































Epoch 6: 100%|████████████████████████████████████████| 2494/2494 [40:58<00:00,  1.01it/s, v_num=reup, train_loss=-1.02e+3, train_loss_step=-, train_loss_epoch=-2.53e+3, val_loss=-6.7e+6]























































Epoch 6: 100%|████████████████████████████████████████| 2494/2494 [42:47<00:00,  1.03s/it, v_num=reup, train_loss=-1.02e+3, train_loss_step=-, train_loss_epoch=-2.53e+3, val_loss=-3.5e+5]-6699657.375061035
































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































Epoch 7: 100%|████████████████████████████████████████| 2494/2494 [37:25<00:00,  1.11it/s, v_num=reup, train_loss=-1.01e+3, train_loss_step=-, train_loss_epoch=-2.53e+3, val_loss=-6.7e+6]






















































Epoch 7: 100%|████████████████████████████████████████| 2494/2494 [39:14<00:00,  1.06it/s, v_num=reup, train_loss=-1.01e+3, train_loss_step=-, train_loss_epoch=-2.53e+3, val_loss=-3.5e+5]-6703150.800666809













































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































Epoch 8:  89%|███████████████████████████████████▌    | 2220/2494 [36:53<04:33,  1.00it/s, v_num=reup, train_loss=-1.74e+3, train_loss_step=-, train_loss_epoch=-2.53e+3, val_loss=-6.7e+6]
/home/andrea/miniconda3/envs/cla-transformer/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:52: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
  rank_zero_warn("Detected KeyboardInterrupt, attempting graceful shutdown...")