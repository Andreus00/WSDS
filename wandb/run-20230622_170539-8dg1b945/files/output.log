Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[34m[1mwandb[39m[22m: logging graph, to disable use `wandb.watch(log_graph=False)`
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
/home/andrea/miniconda3/envs/cla-transformer/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:615: UserWarning: Checkpoint directory /home/andrea/Documenti/Computer Science/Natural Language Procesing/nlp2023-hw2/model exists and is not empty.
  rank_zero_warn(f"Checkpoint directory {dirpath} exists and is not empty.")
Sanity Checking DataLoader 0:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                                                                                  | 1/2 [00:00<00:00,  1.06it/s]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
  | Name            | Type             | Params
-----------------------------------------------------
0 | gloss_encoder   | GlossEncoder     | 108 M
1 | context_encoder | ContextEncoder   | 108 M
2 | loss            | CrossEntropyLoss | 0
3 | cc              | CosineSimilarity | 0
-----------------------------------------------------
216 M     Trainable params
0         Non-trainable params
216 M     Total params
866.482   Total estimated model params size (MB)
/home/andrea/miniconda3/envs/cla-transformer/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/andrea/miniconda3/envs/cla-transformer/lib/python3.10/site-packages/pytorch_lightning/utilities/data.py:120: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-process data loading (when num_workers > 1), `__len__` could be inaccurate if each worker is not configured independently to avoid having duplicate data.
  rank_zero_warn(
/home/andrea/miniconda3/envs/cla-transformer/lib/python3.10/site-packages/pytorch_lightning/utilities/data.py:76: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 8. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
Epoch 0:   0%|                                                                                                                                                                                                                                                               | 0/2494 [00:00<?, ?it/s]tensor([[0.3334, 0.3348, 0.3115, 0.3565, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000],
        [0.1867, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000],
        [0.1815, 0.3008, 0.1552, 0.2262, 0.1468, 0.2553, 0.2334, 0.2408, 0.1592,
         0.2824, 0.1914],
        [0.1957, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000],
        [0.4493, 0.4159, 0.3557, 0.3372, 0.3299, 0.2946, 0.3077, 0.0000, 0.0000,
         0.0000, 0.0000],
        [0.2677, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000],
        [0.3889, 0.2953, 0.3149, 0.3640, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000],
        [0.2841, 0.2739, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000]], device='cuda:0', grad_fn=<StackBackward0>)
Epoch 0:   0%|                                                                                                                                                                                                                   | 1/2494 [00:01<43:21,  1.04s/it, v_num=b945, train_loss_batch=27.60]
/home/andrea/miniconda3/envs/cla-transformer/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
Epoch 0:   0%|                                                                                                                                                                                                                   | 1/2494 [00:01<43:21,  1.04s/it, v_num=b945, train_loss_batch=27.60]tensor([[ 0.2989,  0.3369,  0.3613,  0.2877,  0.2886,  0.3012,  0.3451,  0.2805,
          0.2791],
        [ 0.1067,  0.0796,  0.1195,  0.0996,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000],
        [ 0.2848,  0.3248,  0.3142,  0.2618,  0.3788,  0.3889,  0.3253,  0.2806,
          0.0000],
        [ 0.2851,  0.3152,  0.2600,  0.1863,  0.2986,  0.2866,  0.2779,  0.2403,
          0.0000],
        [ 0.2368,  0.2710,  0.2759,  0.2186,  0.2538,  0.0000,  0.0000,  0.0000,
          0.0000],
        [ 0.3053,  0.2295,  0.2873,  0.2060,  0.2232,  0.2369,  0.1844,  0.2179,
          0.1364],
        [-0.0303, -0.0373, -0.0258,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000],
        [ 0.4185,  0.3970,  0.3989,  0.3408,  0.4636,  0.0000,  0.0000,  0.0000,
          0.0000]], device='cuda:0', grad_fn=<StackBackward0>)
Epoch 0:   0%|‚ñè                                                                                                                                                                                                                  | 2/2494 [00:01<40:24,  1.03it/s, v_num=b945, train_loss_batch=31.70]tensor([[ 0.2152,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.2289,  0.3304,  0.2965,  0.3538,  0.3373,  0.3365],
        [ 0.2454,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.2180,  0.2091,  0.0000,  0.0000,  0.0000,  0.0000],
        [-0.1266, -0.1075, -0.0880,  0.0000,  0.0000,  0.0000],
        [ 0.3116,  0.3484,  0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.1635,  0.1399,  0.1475,  0.1825,  0.2064,  0.1301],
        [ 0.3623,  0.3669,  0.3973,  0.0000,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<StackBackward0>)
Epoch 0:   0%|‚ñé                                                                                                                                                                                                                  | 3/2494 [00:02<35:39,  1.16it/s, v_num=b945, train_loss_batch=25.50]tensor([[0.3081, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000],
        [0.3034, 0.3082, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000],
        [0.3053, 0.2273, 0.3238, 0.2389, 0.2693, 0.2129, 0.3091, 0.2770, 0.0000,
         0.0000],
        [0.2390, 0.2177, 0.2445, 0.2033, 0.2298, 0.2693, 0.3094, 0.1870, 0.1831,
         0.1731],
        [0.0780, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000],
        [0.2102, 0.1897, 0.1617, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000],
        [0.3694, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000],
        [0.1760, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000]], device='cuda:0', grad_fn=<StackBackward0>)
Epoch 0:   0%|‚ñé                                                                                                                                                                                                                  | 4/2494 [00:03<36:16,  1.14it/s, v_num=b945, train_loss_batch=26.40]tensor([[0.3357, 0.2122, 0.2386, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000],
        [0.2706, 0.3587, 0.3631, 0.3322, 0.3786, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000],
        [0.2103, 0.2363, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000],
        [0.2138, 0.2994, 0.2192, 0.2118, 0.2905, 0.3292, 0.2547, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000],
        [0.1646, 0.2938, 0.2069, 0.2826, 0.1807, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000],
        [0.3022, 0.2906, 0.3282, 0.3083, 0.3306, 0.2886, 0.3237, 0.3298, 0.3703,
         0.3196, 0.4171, 0.3081, 0.3332],
        [0.1610, 0.1925, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000],
        [0.3176, 0.2929, 0.3067, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:0',
       grad_fn=<StackBackward0>)
Epoch 0:   0%|‚ñç                                                                                                                                                                                                                  | 5/2494 [00:04<38:38,  1.07it/s, v_num=b945, train_loss_batch=29.70]tensor([[0.1628, 0.2340, 0.1448, 0.1796, 0.1576, 0.2239, 0.0000],
        [0.2051, 0.2036, 0.2796, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.2208, 0.2969, 0.2399, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.2386, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.2446, 0.2439, 0.2253, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1293, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.2007, 0.2182, 0.2017, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.2704, 0.2399, 0.2314, 0.2314, 0.1932, 0.2314, 0.3150]],
       device='cuda:0', grad_fn=<StackBackward0>)
Epoch 0:   0%|‚ñå                                                                                                                                                                                                                  | 6/2494 [00:05<37:05,  1.12it/s, v_num=b945, train_loss_batch=26.30]tensor([[0.3194, 0.2740, 0.2693, 0.3789, 0.2360, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000],
        [0.2447, 0.3617, 0.1666, 0.2962, 0.2471, 0.2574, 0.2472, 0.3285, 0.2620,
         0.2724, 0.1546],
        [0.1619, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000],
        [0.2480, 0.2193, 0.3257, 0.2125, 0.2581, 0.2719, 0.2918, 0.2165, 0.2360,
         0.2232, 0.0000],
        [0.2675, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000],
        [0.2650, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000],
        [0.1815, 0.1950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000],
        [0.3538, 0.4003, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000]], device='cuda:0', grad_fn=<StackBackward0>)
Epoch 0:   0%|‚ñå                                                                                                                                                                                                                  | 7/2494 [00:06<37:47,  1.10it/s, v_num=b945, train_loss_batch=28.10]tensor([[0.4451, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000],
        [0.2901, 0.2664, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000],
        [0.3113, 0.3314, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000],
        [0.0996, 0.0866, 0.1060, 0.0498, 0.0431, 0.0795, 0.0548, 0.0661, 0.0202,
         0.0820, 0.0000, 0.0000, 0.0000],
        [0.2542, 0.3203, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000],
        [0.2903, 0.3365, 0.4851, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000],
        [0.2438, 0.3246, 0.2406, 0.3008, 0.2983, 0.2605, 0.2588, 0.2240, 0.1841,
         0.2683, 0.2532, 0.2987, 0.2752],
        [0.3360, 0.3504, 0.3171, 0.3377, 0.3448, 0.3310, 0.4165, 0.2792, 0.2979,
         0.2695, 0.3315, 0.0000, 0.0000]], device='cuda:0',
       grad_fn=<StackBackward0>)
Epoch 0:   0%|‚ñã                                                                                                                                                                                                                  | 8/2494 [00:07<39:31,  1.05it/s, v_num=b945, train_loss_batch=29.70]tensor([[0.2490, 0.1914, 0.1406, 0.2733, 0.2480, 0.2196, 0.2221, 0.0000],
        [0.3526, 0.3017, 0.3772, 0.3811, 0.3614, 0.3095, 0.3052, 0.3155],
        [0.3358, 0.2936, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.3364, 0.3275, 0.3059, 0.3369, 0.2940, 0.0000, 0.0000, 0.0000],
        [0.2547, 0.1637, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0999, 0.1688, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.2898, 0.3506, 0.2767, 0.2785, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.2976, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<StackBackward0>)
Epoch 0:   0%|‚ñä                                                                                                                                                                                                                  | 9/2494 [00:08<39:02,  1.06it/s, v_num=b945, train_loss_batch=27.60]
       device='cuda:0', grad_fn=<StackBackward0>)0.0469, 0.0784, 0.0678, 0.1076,                                                                                                                                                 | 8/2494 [00:07<39:31,  1.05it/s, v_num=b945, train_loss_batch=29.70]tensor([[0.2490, 0.1914, 0.1406, 0.2733, 0.2480, 0.2196, 0.2221, 0.0000],
         0.0955, 0.1456, 0.0944, 0.1206, 0.0610, 0.0952, 0.0336, 0.0241],
        [0.2586, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.2926, 0.3301, 0.2717, 0.2254, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1415, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<StackBackward0>)
Epoch 0:   0%|‚ñä                                                                                                                                                                                                                 | 10/2494 [00:10<41:38,  1.01s/it, v_num=b945, train_loss_batch=26.20]tensor([[0.2876, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1059, 0.0889, 0.1509, 0.2154, 0.1203, 0.0000, 0.0000],
        [0.1919, 0.1637, 0.2153, 0.2755, 0.2982, 0.2864, 0.2080],
        [0.2205, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1258, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.3185, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.2719, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.2816, 0.2837, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<StackBackward0>)0.0469, 0.0784, 0.0678, 0.1076,                                                                                                                                                 | 8/2494 [00:07<39:31,  1.05it/s, v_num=b945, train_loss_batch=29.70]tensor([[0.2490, 0.1914, 0.1406, 0.2733, 0.2480, 0.2196, 0.2221, 0.0000],
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000],                                                                                                                                                                               | 11/2494 [00:10<40:38,  1.02it/s, v_num=b945, train_loss_batch=23.40]tensor([[0.3174, 0.2726, 0.2760, 0.3012, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1705, 0.2174, 0.2448, 0.3323, 0.2427, 0.2810, 0.3438, 0.2510, 0.3067,
         0.2726, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.2172, 0.2739, 0.3095, 0.2330, 0.2145, 0.2382, 0.2224, 0.2404, 0.2182,
         0.2393, 0.2595, 0.1711, 0.2506, 0.2607, 0.2044, 0.2867],
        [0.1002, 0.2181, 0.2532, 0.2304, 0.2401, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0897, 0.0999, 0.2320, 0.1688, 0.1994, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1149, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1532, 0.1477, 0.1839, 0.2123, 0.1447, 0.2056, 0.2171, 0.1935, 0.1585,
         0.1441, 0.1922, 0.1692, 0.1929, 0.0000, 0.0000, 0.0000],
        [0.2445, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<StackBackward0>)
Epoch 0:   0%|‚ñà                                                                                                                                                                                                                 | 12/2494 [00:12<42:02,  1.02s/it, v_num=b945, train_loss_batch=31.90]tensor([[0.3230, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000],                                                                                                                                                                               | 11/2494 [00:10<40:38,  1.02it/s, v_num=b945, train_loss_batch=23.40]tensor([[0.3174, 0.2726, 0.2760, 0.3012, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        [0.2154, 0.1935, 0.2033, 0.2037, 0.1799, 0.2220, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.2217, 0.2276, 0.3012, 0.2026, 0.2007, 0.2094, 0.2363, 0.1680, 0.2958,
         0.3578, 0.3226, 0.2893, 0.2144, 0.2366],
        [0.1215, 0.1120, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.3540, 0.3049, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1786, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.3428, 0.4031, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.3053, 0.3217, 0.2710, 0.4090, 0.2834, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:0',
       grad_fn=<StackBackward0>)
Epoch 0:   1%|‚ñà                                                                                                                                                                                                                 | 13/2494 [00:13<42:52,  1.04s/it, v_num=b945, train_loss_batch=27.90]tensor([[0.3278, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1785, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1254, 0.1747, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1991, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0545, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.3740, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.2950, 0.3085, 0.2478, 0.3542, 0.3357, 0.3359, 0.3250],
        [0.3190, 0.3862, 0.2243, 0.3425, 0.2724, 0.4057, 0.3079]],
       device='cuda:0', grad_fn=<StackBackward0>)
Epoch 0:   1%|‚ñà‚ñè                                                                                                                                                                                                                | 14/2494 [00:14<41:54,  1.01s/it, v_num=b945, train_loss_batch=24.60]tensor([[0.3131, 0.2335, 0.2755, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.4081, 0.4460, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1396, 0.1545, 0.1316, 0.1653, 0.1237, 0.1501, 0.1497, 0.1104],
        [0.1817, 0.1920, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0755, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.2798, 0.2610, 0.3358, 0.2920, 0.3320, 0.2313, 0.0000, 0.0000],
        [0.2693, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.2896, 0.2877, 0.2642, 0.2882, 0.2118, 0.2885, 0.2907, 0.0000]],
       device='cuda:0', grad_fn=<StackBackward0>)
Epoch 0:   1%|‚ñà‚ñé                                                                                                                                                                                                                | 15/2494 [00:14<41:13,  1.00it/s, v_num=b945, train_loss_batch=27.20]
Epoch 0:   1%|‚ñà                                                                                                                                                                                                                 | 13/2494 [00:13<42:52,  1.04s/it, v_num=b945, train_loss_batch=27.90]tensor([[0.3278, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],